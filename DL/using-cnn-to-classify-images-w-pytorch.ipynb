{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Convolutional Neural Networks (CNN) to classify Images\n",
    "\n",
    "We will use the **[PyTorch](https://pytorch.org/)** open source Python distributio  to define a Convolutional Neaural Network that will be trained on the Natural Images dataset [1] by *Prasun Roy*.\n",
    "\n",
    "\n",
    "## Convolutional Neural Network (CNN)\n",
    "\n",
    "A **convolutional neural network (CNN)** consists of an input and an output layer, as well as multiple hidden layers. The hidden layers of a CNN typically consist of a series of **convolutional layers** that convolve with a multiplication or other dot product. The activation function is commonly a *REctified Linear Unit (RELU) layer*, and is subsequently followed by additional convolutions such as **pooling layers**, **fully connected layers** and **normalization layers**, referred to as *hidden layers* because their inputs and outputs are masked by the activation function and final convolution. The final convolution, in turn, often involves backpropagation in order to more accurately weight the end product.[2]\n",
    "\n",
    "To recap, a CNN have\n",
    "1. Convolutional Layers\n",
    "2. Pooling Layers\n",
    "3. Fully Connected Layers\n",
    "4. Nornalization Layers\n",
    "where 2,3,4 are *hidden layers*. \n",
    "\n",
    "![a](https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png)\n",
    "\n",
    "\n",
    "\n",
    "### Convolutional Layers \n",
    "\n",
    "When programming a CNN, the input is a tensor with shape (number of images, (image width , image height), image depth). Then after passing through a convolutional layer, the image becomes abstracted to a feature map, with shape (number of images, (feature map width, feature map height) , feature map channels). A **convolutional layer** within a neural network should have the following attributes:\n",
    "    * Convolutional kernels defined by a width and height (hyper-parameters).\n",
    "    * The number of input channels and output channels (hyper-parameter).\n",
    "    * The depth of the Convolution filter (the input channels) must be equal to the number channels (depth) of the input feature map.\n",
    "\n",
    "Convolutional layers convolve the input and pass its result to the next layer. The convolution operation brings a solution to the problem arising from the presence of a huge number of input data (i.e. the number of pixels) as it reduces the number of free parameters, allowing the network to be deeper with fewer parameters.\n",
    "\n",
    "The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the entries of the filter and the input and producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input. \n",
    "\n",
    "Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input and shares parameters with neurons in the same activation map.\n",
    "\n",
    "### Pooling \n",
    "\n",
    "Convolutional networks may include **local** or **global pooling layers** to streamline the underlying computation. Pooling layers *reduce the dimensions of the data* by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, typically 2 x 2. \n",
    "\n",
    "Global pooling acts on all the neurons of the convolutional layer. In addition, pooling may compute a max or an average. *Max pooling* uses the maximum value from each of a cluster of neurons at the prior layer. *Average pooling* uses the average value from each of a cluster of neurons at the prior layer.\n",
    "\n",
    "![a](https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png)\n",
    "\n",
    "### Fully Connected Layers\n",
    "\n",
    "**Fully connected layers** connect every neuron in one layer to every neuron in another layer. It is in principle the same as the traditional multi-layer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.\n",
    "\n",
    "#### Receptive Fields\n",
    "\n",
    "In neural networks, each neuron receives input from some number of locations in the previous layer. In a *fully connected layer*, *each* neuron receives input from every element of the previous layer. In a *convolutional layer*, neurons receive input from *only a restricted subarea* of the previous layer. Typically the subarea is of a square shape (e.g., size 5 by 5). The input area of a neuron is called its **receptive field**. \n",
    "\n",
    "So: \n",
    "\n",
    "1. in a **fully connected layer**, the receptive field is the *entire previous layer*. \n",
    "2. In a **convolutional layer**, the receptive area is smaller than the entire previous layer.\n",
    "\n",
    "### Weights\n",
    "\n",
    "Each neuron in a neural network computes an output value by applying a specific function to the input values coming from the receptive field in the previous layer (e.g. *perceptron* [3]). The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning, in a neural network, progresses by making iterative adjustments to these biases and weights.\n",
    "\n",
    "The vector of weights and the bias are called **filters** and represent particular features of the input (e.g., a particular shape). \n",
    "\n",
    "A distinguishing feature of CNNs is that many neurons can *share the same filter*. This reduces memory footprint because a single bias and a single vector of weights are used across all receptive fields sharing that filter, as opposed to each receptive field having its own bias and vector weighting. \n",
    "\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "**Backpropagation** is an algorithm widely used in the training of feedforward neural networks for supervised learning; generalizations exist for other artificial neural networks (ANNs), and for functions generally. Backpropagation efficiently computes the gradient of the loss function with respect to the weights of the network for a single input-output example. This makes it feasible to use gradient methods for training multi-layer networks, updating weights to minimize loss; commonly one uses gradient descent or variants such as stochastic gradient descent. The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, iterating backwards one layer at a time from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic programming. [4], [5].\n",
    "\n",
    "When we train a CNN, we perform mulitple passes forward through the network of layers, and then use a *loss function* to measure the difference between the output values (which you may recall are probability predictions for each class) and the actual values for the known image classes used to train the model (in other words, 1 for the correct class and 0 for all the others). For example, if our CNN have three possibile classes $[C_1, C_2, C_3]$ and, e.g.,  predicted probabilities are 0.15 for $C_1$, 0.8 for $C_2$, and 0.05 for $C_2$. Let's suppose that the image in question is an example of $C_2$, so the expected output is actually 0 for$C_1$, 1 for $C_2$, and 0 for $C_3$. The error (or *loss*) represents how far from the expected values our results are.\n",
    "\n",
    "Having calculated the loss, the training process uses a specified *optimizer* to calculate the derivitive of the loss function wit respect to the weights and biases used in the network layers, and determine how best to adjust them to reduce the loss. We then go backwards through the network, adjusting the weights before the next forward pass. The degree to which we adjust the weights is determined by the *learning rate* - the larger the learning rate, the bigger the adjustments made to the weights.\n",
    "\n",
    "### Counteracting Overfitting: Data Augmentation and Drop Layers\n",
    "\n",
    "#### Overfitting\n",
    "\n",
    "In statistics, overfitting is \n",
    "> the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably.\n",
    "\n",
    "An overfitted model is a statistical model that contains more parameters than can be justified by the data. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e. the noise) as if that variation represented underlying model structure. [6] \n",
    "\n",
    "![b](https://upload.wikimedia.org/wikipedia/commons/1/1f/Overfitting_svg.svg)\n",
    "\n",
    "In the image above: *training error* is shown in blue, *validation error* in red, both as a function of the number of training epochs. If the validation error increases (positive slope) while the training error steadily decreases (negative slope), then a situation of overfitting may have occurred. \n",
    "\n",
    "The best predictive and fitted model would be where the validation error has its global minimum.\n",
    "\n",
    "#### Data Augmentation\n",
    "\n",
    "One way to mitigate the overfitting problem is to perform *data augmentation* by making random transformations of the training images; for example by *flipping*, *rotating*, or *cropping* the images. \n",
    "\n",
    "Because these data augmentation transformations are randomly applied during training, the same image might be presented differently from batch to batch, creating more variation in the training data and helping the model to learn features based the same objects at different orientations or scales.\n",
    "\n",
    "#### Drop Layers\n",
    "\n",
    "During the training process, the convolution and pooling layers in the feature extraction section of the model generate lots of feature maps from the training images. Randomly dropping some of these feature maps helps vary the features that are extracted in each batch, ensuring the model doesn't become overly-reliant on any one dominant feature in the training data. [7]\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "### App. A: Basics of Artificial Neural Networks\n",
    "\n",
    "**Artificial neural networks** (ANN or NN)  are computing systems that are inspired by, but not identical to, biological neural networks that constitute animal brains. Such systems *learn* to perform tasks by considering examples, generally without being programmed with task-specific rules.\n",
    "\n",
    "A NN is based on a collection of connected units or nodes called **artificial neurons**, which loosely model the neurons in a biological brain. The basic example is the **perceptron**[a] . Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it.\n",
    "\n",
    "![a](https://www.allaboutcircuits.com/uploads/articles/how-to-perform-classification-using-a-neural-network-a-simple-perceptron-example_rk_aac_image2.jpg)\n",
    "\n",
    "In ANN implementations, the \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times. [b], \n",
    "\n",
    "#### Single-layer and Multi-layer perceptrons\n",
    "\n",
    "A **single layer perceptron (SLP)** is a feed-forward network based on a threshold transfer function. SLP is the simplest type of artificial neural networks and can only classify linearly separable cases with a binary target (1, 0). [c], [d]\n",
    "\n",
    "Because SLP is a linear classifier and if the cases are not linearly separable the learning process will never reach a point where all the cases are classified properly. The most famous example of the inability of perceptron to solve problems with linearly non-separable cases is the XOR problem.\n",
    "\n",
    "A **multi-layer perceptron (MLP)** has the same structure of a single layer perceptron with one or more hidden layers. The backpropagation algorithm consists of two phases: the forward phase where the activations are propagated from the input to the output layer, and the backward phase, where the error between the observed actual and the requested nominal value in the output layer is propagated backwards in order to modify the weights and bias values. \t\n",
    "\n",
    "![a](https://www.saedsayad.com/images/Perceptron_bkp_1.png)\n",
    "\n",
    "-----\n",
    "\n",
    "### About the Dataset\n",
    "#### Natural Images\n",
    "This dataset is created as a benchmark dataset for the work on Effects of Degradations on Deep Neural Network Architectures.\n",
    "The source code is publicly available on GitHub.\n",
    "\n",
    "#### Description\n",
    "This dataset contains 6,899 images from 8 distinct classes compiled from various sources (see Acknowledgements). The classes include airplane, car, cat, dog, flower, fruit, motorbike and person.\n",
    "\n",
    "#### Acknowledgements\n",
    "\n",
    "1. Airplane images obtained from http://host.robots.ox.ac.uk/pascal/VOC\n",
    "2. Car images obtained from https://ai.stanford.edu/~jkrause/cars/car_dataset.html\n",
    "3. Cat images obtained from https://www.kaggle.com/c/dogs-vs-cats\n",
    "4. Dog images obtained from https://www.kaggle.com/c/dogs-vs-cats\n",
    "5. Flower images obtained from http://www.image-net.org\n",
    "6. Fruit images obtained from https://www.kaggle.com/moltean/fruits\n",
    "7. Motorbike images obtained from http://host.robots.ox.ac.uk/pascal/VOC\n",
    "8. Person images obtained from http://www.briancbecker.com/blog/research/pubfig83-lfw-dataset\n",
    "\n",
    "\n",
    "-----\n",
    "[1] https://www.kaggle.com/prasunroy/natural-images\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Convolutional_neural_network\n",
    "\n",
    "[3] https://en.wikipedia.org/wiki/Perceptron\n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/Backpropagation\n",
    "\n",
    "[5] https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199\n",
    "\n",
    "[6] https://en.wikipedia.org/wiki/Overfitting\n",
    "\n",
    "[7] https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/\n",
    "\n",
    "[a] https://en.wikipedia.org/wiki/Perceptron\n",
    "\n",
    "[b] https://en.wikipedia.org/wiki/Artificial_neural_network\n",
    "\n",
    "[c] https://www.saedsayad.com/artificial_neural_network_bkp.htm\n",
    "\n",
    "[d] https://iamtrask.github.io/2015/07/12/basic-python-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as mp_image\n",
    "import seaborn as sns\n",
    "\n",
    "# Required magic to display matplotlib plots in notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'car', 'cat', 'dog', 'flower', 'fruit', 'motorbike', 'person']\n"
     ]
    }
   ],
   "source": [
    "# The images are in a folder named 'input/natural-images/natural_images'\n",
    "training_folder_name = '../input/natural-images/data/natural_images'\n",
    "\n",
    "# All images are 128x128 pixels\n",
    "img_size = (128,128)\n",
    "\n",
    "# The folder contains a subfolder for each class of shape\n",
    "classes = sorted(os.listdir(training_folder_name))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported - ready to use PyTorch 1.3.0\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* # Preliminaries I: standardize the images' size\n",
    "\n",
    "1. We have a huge amount of images with different sizes and shapes. We will this define a resizing function **resize_image** that resize consistently the image to a shape passed to the function by the user (by default is (128,128)), as done in my notebook on [image pre-treatment](https://www.kaggle.com/androbomb/image-pre-treatment). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# function to resize image\n",
    "def resize_image(src_image, size=(128,128), bg_color=\"white\"): \n",
    "    from PIL import Image, ImageOps \n",
    "    \n",
    "    # resize the image so the longest dimension matches our target size\n",
    "    src_image.thumbnail(size, Image.ANTIALIAS)\n",
    "    \n",
    "    # Create a new square background image\n",
    "    new_image = Image.new(\"RGB\", size, bg_color)\n",
    "    \n",
    "    # Paste the resized image into the center of the square background\n",
    "    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n",
    "  \n",
    "    # return the resized image\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming images...\n",
      "processing folder airplane\n",
      "processing folder dog\n",
      "processing folder fruit\n",
      "processing folder cat\n",
      "processing folder flower\n",
      "processing folder person\n",
      "processing folder car\n",
      "processing folder motorbike\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "training_folder_name = '../input/natural-images/data/natural_images'\n",
    "\n",
    "# New location for the resized images\n",
    "train_folder = '../working/data/natural_images'\n",
    "\n",
    "\n",
    "# Create resized copies of all of the source images\n",
    "size = (128,128)\n",
    "\n",
    "# Create the output folder if it doesn't already exist\n",
    "if os.path.exists(train_folder):\n",
    "    shutil.rmtree(train_folder)\n",
    "\n",
    "# Loop through each subfolder in the input folder\n",
    "print('Transforming images...')\n",
    "for root, folders, files in os.walk(training_folder_name):\n",
    "    for sub_folder in folders:\n",
    "        print('processing folder ' + sub_folder)\n",
    "        # Create a matching subfolder in the output dir\n",
    "        saveFolder = os.path.join(train_folder,sub_folder)\n",
    "        if not os.path.exists(saveFolder):\n",
    "            os.makedirs(saveFolder)\n",
    "        # Loop through the files in the subfolder\n",
    "        file_names = os.listdir(os.path.join(root,sub_folder))\n",
    "        for file_name in file_names:\n",
    "            # Open the file\n",
    "            file_path = os.path.join(root,sub_folder, file_name)\n",
    "            #print(\"reading \" + file_path)\n",
    "            image = Image.open(file_path)\n",
    "            # Create a resized version and save it\n",
    "            resized_image = resize_image(image, size)\n",
    "            saveAs = os.path.join(saveFolder, file_name)\n",
    "            #print(\"writing \" + saveAs)\n",
    "            resized_image.save(saveAs)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries II: prepare and augment the data\n",
    "\n",
    "PyTorch [1]  includes functions for loading and transforming data, as **[torchvision.datasets.ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html)** .\n",
    "\n",
    "We will use these to create an iterative loader for *training data*, and a second iterative loader for *test data*. \n",
    "\n",
    "The loaders will transform the image data into *tensors*, which are the core data structure used in PyTorch, and normalize them so that the pixel values are in a scale with a mean of 0.5 and a standard deviation of 0.5.\n",
    "\n",
    "At this point we can add transformations to randomly modify the images as they are added to a training batch. In this case, we will flip images horizontally at random. This is done to prevent **overfitting**. The transformations are done using **torchvision.transforms** [2]. The basic transformations available are\n",
    "1. transforms.RandomVerticalFlip(p=0.5)\n",
    "2. transforms.RandomHorizontalFlip(p=0.5)\n",
    "3. transforms.RandomRotation(degrees, resample=False, expand=False, center=None, fill=0)\n",
    "4. transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)\n",
    "    * Crop the given PIL Image to random size and aspect ratio.\n",
    "\n",
    "-----\n",
    "[1] https://pytorch.org/\n",
    "\n",
    "[2] https://pytorch.org/docs/stable/torchvision/transforms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders ready to read ../working/data/natural_images\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(data_path):\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import torchvision.transforms as transforms\n",
    "    # Load all the images\n",
    "    transformation = transforms.Compose([\n",
    "        # Randomly augment the image data\n",
    "            # Random horizontal flip\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "            # Random vertical flip\n",
    "        transforms.RandomVerticalFlip(0.3),\n",
    "        # transform to tensors\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the pixel values (in R, G, and B channels)\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=transformation\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Split into training (70% and testing (30%) datasets)\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    \n",
    "    # use torch.utils.data.random_split for training/test split\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # define a loader for the training data we can iterate through in 50-image batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=50,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # define a loader for the testing data we can iterate through in 50-image batches\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=50,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "        \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Recall that we have resized the images and saved them into\n",
    "train_folder = '../working/data/natural_images'\n",
    "\n",
    "# Get the iterative dataloaders for test and training data\n",
    "train_loader, test_loader = load_dataset(train_folder)\n",
    "batch_size = train_loader.batch_size\n",
    "print(\"Data loaders ready to read\", train_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Convolutional Neural Network\n",
    "\n",
    "In PyTorch, you define a neural network model as a class that is derived from the **[nn.Module](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html)** base class. \n",
    "\n",
    "Your class must define the layers in the network, and provide a *forward* method that is used to process data through the layers of the network.\n",
    "\n",
    "As in the kernel [MNIST_ale_CNN](https://www.kaggle.com/androbomb/mnist-ale-cnn), where we used keras to create a CNN to apply it to the standard MNIST database, we build a CNN as following:\n",
    "\n",
    "We now need to create our Convolutional Neural Network model. In order to do so, we need to choose the convolution and poolying layers.\n",
    "\n",
    "We will thus:\n",
    "\n",
    "1. Define the model as a sequential layers\n",
    "2. Introduce the Convolutions. \n",
    "    * they are important to create a feature map\n",
    "3. Introduce the Poolings\n",
    "    * MaxPooling is used to reduce dimensionality. In MaxPooling, the output value is just the maximum of the input values in each patch (for ex. The maximum pixel in a span of 3 pixels).\n",
    "4. Flatten the data in order to have a np.array to feed the NN\n",
    "\n",
    "In particoular, we will apply\n",
    "\n",
    "1. 16 3x3 Convolution matrix with activation='relu', i.e. a Rectified Linear Unit (ReLU) activation.\n",
    "2. A 2x2 MaxPooling\n",
    "3. 32 3x3 Convolution matrix with activation='relu', i.e. a Rectified Linear Unit (ReLU) activation.\n",
    "4. A 2x2 MaxPooling\n",
    "5. 64 3x3 Convolution matrix with activation='relu', i.e. a Rectified Linear Unit (ReLU) activation.\n",
    "6. A 2x2 MaxPooling\n",
    "7. 64 3x3 Convolution matrix with activation='relu', i.e. a Rectified Linear Unit (ReLU) activation.\n",
    "8. A 2x2 MaxPooling\n",
    "\n",
    "As a optimaizer, we decided to use the **ADAM (ADAptive Moment estimation)** optimization algorithm, that is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing. For more on optimizers in PyTorch, see [1]\n",
    "\n",
    "Notice that we have a layer that randomly drops 20% of the features to prevent **overfitting**. \n",
    "\n",
    "-----\n",
    "[1] https://pytorch.org/docs/stable/optim.html#algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout2d(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=24576, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # In the init function, we define each layer we will use in our model\n",
    "        \n",
    "        # Our images are RGB, so we have input channels = 3. \n",
    "        # We will apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We in the end apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # This means that our feature tensors are now 32 x 32, and we've generated 24 of them\n",
    "        \n",
    "        # We need to flatten these in order to feed them to a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        \n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x = F.relu(self.pool(self.conv1(x))) \n",
    "        \n",
    "        # Use a ReLU activation function after layer 2\n",
    "        x = F.relu(self.pool(self.conv2(x)))  \n",
    "        \n",
    "        # Select some features to drop to prevent overfitting (only drop during training)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        # Return class probabilities via a log_softmax function \n",
    "        return torch.log_softmax(x, dim=1)\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=len(classes)).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function\n",
    "\n",
    "Training consists of an iterative series of forward passes in which the training data is processed in batches by the layers in the network, and the optimizer goes back and adjusts the weights. We will also use a separate set of test images to test the model at the end of each *epoch*, so we can track the performance improvement as the training process progresses. \n",
    "\n",
    "\n",
    "\n",
    "The traning function we need to define needs the following steps: \n",
    "\n",
    "1. Set the model to training mode;\n",
    "2. Process the images in batches; we will iterate over images in batches. Inside each batches, we have to\n",
    "    1.  Import labels and features;\n",
    "    2. Reset the optimizer\n",
    "    3. Push the data forward through the layers of the model\n",
    "    4. compute the loss\n",
    "    5. Backpropagate\n",
    "3.  Compute the Average Loss of the Model during the Epoch\n",
    "\n",
    "We will thus calling eath once per epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function: \n",
    "\n",
    "Here we need the model in evaluation mode to get the accuray by confronting with the labels (we don't propagate anything here). So\n",
    "\n",
    "1. Set the model to evaluation mode;\n",
    "2. Process the images in batches; we will iterate over images in batches. Inside each batches, we have to\n",
    "    1. Get the prediction for each image in the batch\n",
    "    2. Calculate the loss for the batch\n",
    "    3. Calculate the accuracy for this batch\n",
    "3. Calculate the average accuracy and loss for the epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "\n",
    "#### Adam optimizer\n",
    "When training the Model, we use the **ADAM** optimizer [1], [2], that is an adaptive learning rate optimization algorithm that’s been designed specifically for training deep neural networks. \n",
    "\n",
    "Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum. Let’s take a closer look at how it works.\n",
    "Adam is an adaptive learning rate method, which means, it computes individual learning rates for different parameters. Its name is derived from adaptive moment estimation, and the reason it’s called that is because Adam uses estimations of first and second moments of gradient to adapt the learning rate for each weight of the neural network.\n",
    "\n",
    "\n",
    "#### Loss Criteria\n",
    "As a loss criteria we use the **Cross Entropy Loss** (log loss), that measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0. [3]\n",
    "\n",
    "\n",
    "------\n",
    "[1] https://arxiv.org/abs/1412.6980\n",
    "\n",
    "[2] https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c\n",
    "\n",
    "[3] https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 2.079145\n",
      "\tTraining batch 2 Loss: 10.118187\n",
      "\tTraining batch 3 Loss: 2.646740\n",
      "\tTraining batch 4 Loss: 2.111869\n",
      "\tTraining batch 5 Loss: 1.669168\n",
      "\tTraining batch 6 Loss: 1.639455\n",
      "\tTraining batch 7 Loss: 1.575683\n",
      "\tTraining batch 8 Loss: 1.656426\n",
      "\tTraining batch 9 Loss: 1.292582\n",
      "\tTraining batch 10 Loss: 1.051112\n",
      "\tTraining batch 11 Loss: 0.823093\n",
      "\tTraining batch 12 Loss: 1.064131\n",
      "\tTraining batch 13 Loss: 0.981580\n",
      "\tTraining batch 14 Loss: 1.005966\n",
      "\tTraining batch 15 Loss: 0.900591\n",
      "\tTraining batch 16 Loss: 0.643629\n",
      "\tTraining batch 17 Loss: 0.826972\n",
      "\tTraining batch 18 Loss: 0.981924\n",
      "\tTraining batch 19 Loss: 0.639102\n",
      "\tTraining batch 20 Loss: 0.750890\n",
      "\tTraining batch 21 Loss: 0.772048\n",
      "\tTraining batch 22 Loss: 0.591403\n",
      "\tTraining batch 23 Loss: 1.003661\n",
      "\tTraining batch 24 Loss: 0.664557\n",
      "\tTraining batch 25 Loss: 0.560226\n",
      "\tTraining batch 26 Loss: 0.617326\n",
      "\tTraining batch 27 Loss: 0.651283\n",
      "\tTraining batch 28 Loss: 0.579034\n",
      "\tTraining batch 29 Loss: 0.610369\n",
      "\tTraining batch 30 Loss: 0.614515\n",
      "\tTraining batch 31 Loss: 0.556791\n",
      "\tTraining batch 32 Loss: 0.779281\n",
      "\tTraining batch 33 Loss: 0.665331\n",
      "\tTraining batch 34 Loss: 0.941620\n",
      "\tTraining batch 35 Loss: 0.748423\n",
      "\tTraining batch 36 Loss: 0.595584\n",
      "\tTraining batch 37 Loss: 0.872631\n",
      "\tTraining batch 38 Loss: 0.649972\n",
      "\tTraining batch 39 Loss: 0.743061\n",
      "\tTraining batch 40 Loss: 0.581584\n",
      "\tTraining batch 41 Loss: 0.589653\n",
      "\tTraining batch 42 Loss: 0.610317\n",
      "\tTraining batch 43 Loss: 0.764243\n",
      "\tTraining batch 44 Loss: 0.643407\n",
      "\tTraining batch 45 Loss: 0.538128\n",
      "\tTraining batch 46 Loss: 0.502962\n",
      "\tTraining batch 47 Loss: 0.556998\n",
      "\tTraining batch 48 Loss: 0.807544\n",
      "\tTraining batch 49 Loss: 0.600101\n",
      "\tTraining batch 50 Loss: 1.116178\n",
      "\tTraining batch 51 Loss: 0.966749\n",
      "\tTraining batch 52 Loss: 0.538882\n",
      "\tTraining batch 53 Loss: 1.037813\n",
      "\tTraining batch 54 Loss: 0.812711\n",
      "\tTraining batch 55 Loss: 0.605267\n",
      "\tTraining batch 56 Loss: 0.492500\n",
      "\tTraining batch 57 Loss: 0.734527\n",
      "\tTraining batch 58 Loss: 0.542652\n",
      "\tTraining batch 59 Loss: 0.550777\n",
      "\tTraining batch 60 Loss: 0.580714\n",
      "\tTraining batch 61 Loss: 0.893581\n",
      "\tTraining batch 62 Loss: 0.656429\n",
      "\tTraining batch 63 Loss: 0.436449\n",
      "\tTraining batch 64 Loss: 0.636412\n",
      "\tTraining batch 65 Loss: 0.516949\n",
      "\tTraining batch 66 Loss: 0.619224\n",
      "\tTraining batch 67 Loss: 0.543289\n",
      "\tTraining batch 68 Loss: 0.761563\n",
      "\tTraining batch 69 Loss: 0.262165\n",
      "\tTraining batch 70 Loss: 0.351301\n",
      "\tTraining batch 71 Loss: 0.569383\n",
      "\tTraining batch 72 Loss: 0.309039\n",
      "\tTraining batch 73 Loss: 0.679443\n",
      "\tTraining batch 74 Loss: 0.300347\n",
      "\tTraining batch 75 Loss: 0.359255\n",
      "\tTraining batch 76 Loss: 0.445596\n",
      "\tTraining batch 77 Loss: 0.535075\n",
      "\tTraining batch 78 Loss: 0.586261\n",
      "\tTraining batch 79 Loss: 0.452938\n",
      "\tTraining batch 80 Loss: 0.538974\n",
      "\tTraining batch 81 Loss: 0.443047\n",
      "\tTraining batch 82 Loss: 0.425782\n",
      "\tTraining batch 83 Loss: 0.853307\n",
      "\tTraining batch 84 Loss: 0.546092\n",
      "\tTraining batch 85 Loss: 0.497170\n",
      "\tTraining batch 86 Loss: 0.536897\n",
      "\tTraining batch 87 Loss: 0.452233\n",
      "\tTraining batch 88 Loss: 0.390050\n",
      "\tTraining batch 89 Loss: 0.429905\n",
      "\tTraining batch 90 Loss: 0.613322\n",
      "\tTraining batch 91 Loss: 0.505289\n",
      "\tTraining batch 92 Loss: 0.830673\n",
      "\tTraining batch 93 Loss: 0.618616\n",
      "\tTraining batch 94 Loss: 0.393771\n",
      "\tTraining batch 95 Loss: 0.314553\n",
      "\tTraining batch 96 Loss: 0.497797\n",
      "\tTraining batch 97 Loss: 0.882109\n",
      "Training set: Average loss: 0.840550\n",
      "Validation set: Average loss: 0.484072, Accuracy: 1683/2070 (81%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.500859\n",
      "\tTraining batch 2 Loss: 0.403541\n",
      "\tTraining batch 3 Loss: 0.322665\n",
      "\tTraining batch 4 Loss: 0.623079\n",
      "\tTraining batch 5 Loss: 0.730446\n",
      "\tTraining batch 6 Loss: 0.452703\n",
      "\tTraining batch 7 Loss: 0.439152\n",
      "\tTraining batch 8 Loss: 0.551467\n",
      "\tTraining batch 9 Loss: 0.476876\n",
      "\tTraining batch 10 Loss: 0.583925\n",
      "\tTraining batch 11 Loss: 0.515809\n",
      "\tTraining batch 12 Loss: 0.696607\n",
      "\tTraining batch 13 Loss: 0.606427\n",
      "\tTraining batch 14 Loss: 0.497284\n",
      "\tTraining batch 15 Loss: 0.518614\n",
      "\tTraining batch 16 Loss: 0.549411\n",
      "\tTraining batch 17 Loss: 0.488326\n",
      "\tTraining batch 18 Loss: 0.441799\n",
      "\tTraining batch 19 Loss: 0.353498\n",
      "\tTraining batch 20 Loss: 0.356803\n",
      "\tTraining batch 21 Loss: 0.747848\n",
      "\tTraining batch 22 Loss: 0.378922\n",
      "\tTraining batch 23 Loss: 0.483365\n",
      "\tTraining batch 24 Loss: 0.535051\n",
      "\tTraining batch 25 Loss: 0.275976\n",
      "\tTraining batch 26 Loss: 0.455312\n",
      "\tTraining batch 27 Loss: 0.459570\n",
      "\tTraining batch 28 Loss: 0.280057\n",
      "\tTraining batch 29 Loss: 0.360904\n",
      "\tTraining batch 30 Loss: 0.494850\n",
      "\tTraining batch 31 Loss: 0.331070\n",
      "\tTraining batch 32 Loss: 0.442507\n",
      "\tTraining batch 33 Loss: 0.417098\n",
      "\tTraining batch 34 Loss: 0.351986\n",
      "\tTraining batch 35 Loss: 0.551342\n",
      "\tTraining batch 36 Loss: 0.514961\n",
      "\tTraining batch 37 Loss: 0.406845\n",
      "\tTraining batch 38 Loss: 0.448985\n",
      "\tTraining batch 39 Loss: 0.567826\n",
      "\tTraining batch 40 Loss: 0.586268\n",
      "\tTraining batch 41 Loss: 0.589390\n",
      "\tTraining batch 42 Loss: 0.357892\n",
      "\tTraining batch 43 Loss: 0.730458\n",
      "\tTraining batch 44 Loss: 0.450937\n",
      "\tTraining batch 45 Loss: 0.317931\n",
      "\tTraining batch 46 Loss: 0.366427\n",
      "\tTraining batch 47 Loss: 0.564286\n",
      "\tTraining batch 48 Loss: 0.656705\n",
      "\tTraining batch 49 Loss: 0.567183\n",
      "\tTraining batch 50 Loss: 0.608944\n",
      "\tTraining batch 51 Loss: 0.532907\n",
      "\tTraining batch 52 Loss: 0.573890\n",
      "\tTraining batch 53 Loss: 0.671675\n",
      "\tTraining batch 54 Loss: 0.682505\n",
      "\tTraining batch 55 Loss: 0.415520\n",
      "\tTraining batch 56 Loss: 0.463177\n",
      "\tTraining batch 57 Loss: 0.680255\n",
      "\tTraining batch 58 Loss: 0.546698\n",
      "\tTraining batch 59 Loss: 0.419342\n",
      "\tTraining batch 60 Loss: 0.404876\n",
      "\tTraining batch 61 Loss: 0.655075\n",
      "\tTraining batch 62 Loss: 0.503780\n",
      "\tTraining batch 63 Loss: 0.355399\n",
      "\tTraining batch 64 Loss: 0.580427\n",
      "\tTraining batch 65 Loss: 0.491493\n",
      "\tTraining batch 66 Loss: 0.604260\n",
      "\tTraining batch 67 Loss: 0.598990\n",
      "\tTraining batch 68 Loss: 0.526515\n",
      "\tTraining batch 69 Loss: 0.361373\n",
      "\tTraining batch 70 Loss: 0.180757\n",
      "\tTraining batch 71 Loss: 0.512389\n",
      "\tTraining batch 72 Loss: 0.498624\n",
      "\tTraining batch 73 Loss: 0.500837\n",
      "\tTraining batch 74 Loss: 0.291114\n",
      "\tTraining batch 75 Loss: 0.643748\n",
      "\tTraining batch 76 Loss: 0.536259\n",
      "\tTraining batch 77 Loss: 0.608149\n",
      "\tTraining batch 78 Loss: 0.238444\n",
      "\tTraining batch 79 Loss: 0.368117\n",
      "\tTraining batch 80 Loss: 0.197065\n",
      "\tTraining batch 81 Loss: 0.378292\n",
      "\tTraining batch 82 Loss: 0.340805\n",
      "\tTraining batch 83 Loss: 0.778266\n",
      "\tTraining batch 84 Loss: 0.367516\n",
      "\tTraining batch 85 Loss: 0.337189\n",
      "\tTraining batch 86 Loss: 0.531741\n",
      "\tTraining batch 87 Loss: 0.338813\n",
      "\tTraining batch 88 Loss: 0.503057\n",
      "\tTraining batch 89 Loss: 0.413331\n",
      "\tTraining batch 90 Loss: 0.340795\n",
      "\tTraining batch 91 Loss: 0.482539\n",
      "\tTraining batch 92 Loss: 0.372160\n",
      "\tTraining batch 93 Loss: 0.640282\n",
      "\tTraining batch 94 Loss: 0.331550\n",
      "\tTraining batch 95 Loss: 0.303775\n",
      "\tTraining batch 96 Loss: 0.274696\n",
      "\tTraining batch 97 Loss: 0.520591\n",
      "Training set: Average loss: 0.477415\n",
      "Validation set: Average loss: 0.350946, Accuracy: 1774/2070 (86%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.257540\n",
      "\tTraining batch 2 Loss: 0.409206\n",
      "\tTraining batch 3 Loss: 0.407004\n",
      "\tTraining batch 4 Loss: 0.342675\n",
      "\tTraining batch 5 Loss: 0.508826\n",
      "\tTraining batch 6 Loss: 0.382474\n",
      "\tTraining batch 7 Loss: 0.686423\n",
      "\tTraining batch 8 Loss: 0.527950\n",
      "\tTraining batch 9 Loss: 0.477615\n",
      "\tTraining batch 10 Loss: 0.479952\n",
      "\tTraining batch 11 Loss: 0.259761\n",
      "\tTraining batch 12 Loss: 0.536799\n",
      "\tTraining batch 13 Loss: 0.413455\n",
      "\tTraining batch 14 Loss: 0.423277\n",
      "\tTraining batch 15 Loss: 0.427262\n",
      "\tTraining batch 16 Loss: 0.337353\n",
      "\tTraining batch 17 Loss: 0.319244\n",
      "\tTraining batch 18 Loss: 0.449768\n",
      "\tTraining batch 19 Loss: 0.300204\n",
      "\tTraining batch 20 Loss: 0.348359\n",
      "\tTraining batch 21 Loss: 0.511105\n",
      "\tTraining batch 22 Loss: 0.280279\n",
      "\tTraining batch 23 Loss: 0.362734\n",
      "\tTraining batch 24 Loss: 0.642320\n",
      "\tTraining batch 25 Loss: 0.279132\n",
      "\tTraining batch 26 Loss: 0.343971\n",
      "\tTraining batch 27 Loss: 0.264261\n",
      "\tTraining batch 28 Loss: 0.393908\n",
      "\tTraining batch 29 Loss: 0.277089\n",
      "\tTraining batch 30 Loss: 0.355102\n",
      "\tTraining batch 31 Loss: 0.292695\n",
      "\tTraining batch 32 Loss: 0.446958\n",
      "\tTraining batch 33 Loss: 0.285436\n",
      "\tTraining batch 34 Loss: 0.433426\n",
      "\tTraining batch 35 Loss: 0.490467\n",
      "\tTraining batch 36 Loss: 0.258278\n",
      "\tTraining batch 37 Loss: 0.198833\n",
      "\tTraining batch 38 Loss: 0.458700\n",
      "\tTraining batch 39 Loss: 0.489646\n",
      "\tTraining batch 40 Loss: 0.422052\n",
      "\tTraining batch 41 Loss: 0.382032\n",
      "\tTraining batch 42 Loss: 0.342880\n",
      "\tTraining batch 43 Loss: 0.734447\n",
      "\tTraining batch 44 Loss: 0.395253\n",
      "\tTraining batch 45 Loss: 0.308454\n",
      "\tTraining batch 46 Loss: 0.422672\n",
      "\tTraining batch 47 Loss: 0.294691\n",
      "\tTraining batch 48 Loss: 0.641815\n",
      "\tTraining batch 49 Loss: 0.221008\n",
      "\tTraining batch 50 Loss: 0.490217\n",
      "\tTraining batch 51 Loss: 0.608624\n",
      "\tTraining batch 52 Loss: 0.347860\n",
      "\tTraining batch 53 Loss: 0.714669\n",
      "\tTraining batch 54 Loss: 0.558532\n",
      "\tTraining batch 55 Loss: 0.269165\n",
      "\tTraining batch 56 Loss: 0.348288\n",
      "\tTraining batch 57 Loss: 0.588835\n",
      "\tTraining batch 58 Loss: 0.396759\n",
      "\tTraining batch 59 Loss: 0.419731\n",
      "\tTraining batch 60 Loss: 0.205552\n",
      "\tTraining batch 61 Loss: 0.901245\n",
      "\tTraining batch 62 Loss: 0.341286\n",
      "\tTraining batch 63 Loss: 0.391415\n",
      "\tTraining batch 64 Loss: 0.456164\n",
      "\tTraining batch 65 Loss: 0.433040\n",
      "\tTraining batch 66 Loss: 0.459996\n",
      "\tTraining batch 67 Loss: 0.397536\n",
      "\tTraining batch 68 Loss: 0.568553\n",
      "\tTraining batch 69 Loss: 0.232473\n",
      "\tTraining batch 70 Loss: 0.485349\n",
      "\tTraining batch 71 Loss: 0.648927\n",
      "\tTraining batch 72 Loss: 0.503711\n",
      "\tTraining batch 73 Loss: 0.730678\n",
      "\tTraining batch 74 Loss: 0.696636\n",
      "\tTraining batch 75 Loss: 0.376854\n",
      "\tTraining batch 76 Loss: 0.468743\n",
      "\tTraining batch 77 Loss: 0.445354\n",
      "\tTraining batch 78 Loss: 0.432097\n",
      "\tTraining batch 79 Loss: 0.392405\n",
      "\tTraining batch 80 Loss: 0.485705\n",
      "\tTraining batch 81 Loss: 0.389353\n",
      "\tTraining batch 82 Loss: 0.374611\n",
      "\tTraining batch 83 Loss: 0.661537\n",
      "\tTraining batch 84 Loss: 0.428417\n",
      "\tTraining batch 85 Loss: 0.274166\n",
      "\tTraining batch 86 Loss: 0.405116\n",
      "\tTraining batch 87 Loss: 0.300040\n",
      "\tTraining batch 88 Loss: 0.251625\n",
      "\tTraining batch 89 Loss: 0.352801\n",
      "\tTraining batch 90 Loss: 0.457028\n",
      "\tTraining batch 91 Loss: 0.563466\n",
      "\tTraining batch 92 Loss: 0.462544\n",
      "\tTraining batch 93 Loss: 0.600696\n",
      "\tTraining batch 94 Loss: 0.375404\n",
      "\tTraining batch 95 Loss: 0.238920\n",
      "\tTraining batch 96 Loss: 0.253730\n",
      "\tTraining batch 97 Loss: 0.763012\n",
      "Training set: Average loss: 0.427563\n",
      "Validation set: Average loss: 0.420697, Accuracy: 1750/2070 (85%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.393525\n",
      "\tTraining batch 2 Loss: 0.454138\n",
      "\tTraining batch 3 Loss: 0.226213\n",
      "\tTraining batch 4 Loss: 0.322373\n",
      "\tTraining batch 5 Loss: 0.576781\n",
      "\tTraining batch 6 Loss: 0.446916\n",
      "\tTraining batch 7 Loss: 0.582393\n",
      "\tTraining batch 8 Loss: 0.684047\n",
      "\tTraining batch 9 Loss: 0.242800\n",
      "\tTraining batch 10 Loss: 0.631147\n",
      "\tTraining batch 11 Loss: 0.375277\n",
      "\tTraining batch 12 Loss: 0.501885\n",
      "\tTraining batch 13 Loss: 0.396951\n",
      "\tTraining batch 14 Loss: 0.282570\n",
      "\tTraining batch 15 Loss: 0.385340\n",
      "\tTraining batch 16 Loss: 0.235694\n",
      "\tTraining batch 17 Loss: 0.378185\n",
      "\tTraining batch 18 Loss: 0.386754\n",
      "\tTraining batch 19 Loss: 0.221702\n",
      "\tTraining batch 20 Loss: 0.489277\n",
      "\tTraining batch 21 Loss: 0.525012\n",
      "\tTraining batch 22 Loss: 0.389093\n",
      "\tTraining batch 23 Loss: 0.463501\n",
      "\tTraining batch 24 Loss: 0.388507\n",
      "\tTraining batch 25 Loss: 0.227244\n",
      "\tTraining batch 26 Loss: 0.425925\n",
      "\tTraining batch 27 Loss: 0.545396\n",
      "\tTraining batch 28 Loss: 0.417371\n",
      "\tTraining batch 29 Loss: 0.189944\n",
      "\tTraining batch 30 Loss: 0.304684\n",
      "\tTraining batch 31 Loss: 0.249082\n",
      "\tTraining batch 32 Loss: 0.249955\n",
      "\tTraining batch 33 Loss: 0.491668\n",
      "\tTraining batch 34 Loss: 0.638747\n",
      "\tTraining batch 35 Loss: 0.519152\n",
      "\tTraining batch 36 Loss: 0.372384\n",
      "\tTraining batch 37 Loss: 0.244784\n",
      "\tTraining batch 38 Loss: 0.550952\n",
      "\tTraining batch 39 Loss: 0.437299\n",
      "\tTraining batch 40 Loss: 0.417638\n",
      "\tTraining batch 41 Loss: 0.517453\n",
      "\tTraining batch 42 Loss: 0.250936\n",
      "\tTraining batch 43 Loss: 0.500562\n",
      "\tTraining batch 44 Loss: 0.302953\n",
      "\tTraining batch 45 Loss: 0.275573\n",
      "\tTraining batch 46 Loss: 0.282381\n",
      "\tTraining batch 47 Loss: 0.378408\n",
      "\tTraining batch 48 Loss: 0.421102\n",
      "\tTraining batch 49 Loss: 0.280862\n",
      "\tTraining batch 50 Loss: 0.404433\n",
      "\tTraining batch 51 Loss: 0.253937\n",
      "\tTraining batch 52 Loss: 0.353731\n",
      "\tTraining batch 53 Loss: 0.422951\n",
      "\tTraining batch 54 Loss: 0.411176\n",
      "\tTraining batch 55 Loss: 0.261734\n",
      "\tTraining batch 56 Loss: 0.384919\n",
      "\tTraining batch 57 Loss: 0.675336\n",
      "\tTraining batch 58 Loss: 0.452683\n",
      "\tTraining batch 59 Loss: 0.362340\n",
      "\tTraining batch 60 Loss: 0.320800\n",
      "\tTraining batch 61 Loss: 0.519193\n",
      "\tTraining batch 62 Loss: 0.422623\n",
      "\tTraining batch 63 Loss: 0.535424\n",
      "\tTraining batch 64 Loss: 0.602059\n",
      "\tTraining batch 65 Loss: 0.541157\n",
      "\tTraining batch 66 Loss: 0.528901\n",
      "\tTraining batch 67 Loss: 0.389615\n",
      "\tTraining batch 68 Loss: 0.413794\n",
      "\tTraining batch 69 Loss: 0.240902\n",
      "\tTraining batch 70 Loss: 0.247156\n",
      "\tTraining batch 71 Loss: 0.522365\n",
      "\tTraining batch 72 Loss: 0.634164\n",
      "\tTraining batch 73 Loss: 0.464204\n",
      "\tTraining batch 74 Loss: 0.531910\n",
      "\tTraining batch 75 Loss: 0.488608\n",
      "\tTraining batch 76 Loss: 0.423669\n",
      "\tTraining batch 77 Loss: 0.596142\n",
      "\tTraining batch 78 Loss: 0.462595\n",
      "\tTraining batch 79 Loss: 0.516443\n",
      "\tTraining batch 80 Loss: 0.240567\n",
      "\tTraining batch 81 Loss: 0.325314\n",
      "\tTraining batch 82 Loss: 0.182844\n",
      "\tTraining batch 83 Loss: 0.667584\n",
      "\tTraining batch 84 Loss: 0.215942\n",
      "\tTraining batch 85 Loss: 0.311111\n",
      "\tTraining batch 86 Loss: 0.252033\n",
      "\tTraining batch 87 Loss: 0.202068\n",
      "\tTraining batch 88 Loss: 0.411534\n",
      "\tTraining batch 89 Loss: 0.218763\n",
      "\tTraining batch 90 Loss: 0.427072\n",
      "\tTraining batch 91 Loss: 0.393088\n",
      "\tTraining batch 92 Loss: 0.507469\n",
      "\tTraining batch 93 Loss: 0.385208\n",
      "\tTraining batch 94 Loss: 0.333316\n",
      "\tTraining batch 95 Loss: 0.295483\n",
      "\tTraining batch 96 Loss: 0.398355\n",
      "\tTraining batch 97 Loss: 0.419821\n",
      "Training set: Average loss: 0.402527\n",
      "Validation set: Average loss: 0.378753, Accuracy: 1762/2070 (85%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.360616\n",
      "\tTraining batch 2 Loss: 0.261287\n",
      "\tTraining batch 3 Loss: 0.310816\n",
      "\tTraining batch 4 Loss: 0.594119\n",
      "\tTraining batch 5 Loss: 0.535547\n",
      "\tTraining batch 6 Loss: 0.398786\n",
      "\tTraining batch 7 Loss: 0.591465\n",
      "\tTraining batch 8 Loss: 0.487197\n",
      "\tTraining batch 9 Loss: 0.469850\n",
      "\tTraining batch 10 Loss: 0.333372\n",
      "\tTraining batch 11 Loss: 0.290380\n",
      "\tTraining batch 12 Loss: 0.495964\n",
      "\tTraining batch 13 Loss: 0.281196\n",
      "\tTraining batch 14 Loss: 0.193382\n",
      "\tTraining batch 15 Loss: 0.347281\n",
      "\tTraining batch 16 Loss: 0.150252\n",
      "\tTraining batch 17 Loss: 0.503765\n",
      "\tTraining batch 18 Loss: 0.503846\n",
      "\tTraining batch 19 Loss: 0.210351\n",
      "\tTraining batch 20 Loss: 0.662900\n",
      "\tTraining batch 21 Loss: 0.487166\n",
      "\tTraining batch 22 Loss: 0.424450\n",
      "\tTraining batch 23 Loss: 0.414640\n",
      "\tTraining batch 24 Loss: 0.241213\n",
      "\tTraining batch 25 Loss: 0.202002\n",
      "\tTraining batch 26 Loss: 0.439005\n",
      "\tTraining batch 27 Loss: 0.340009\n",
      "\tTraining batch 28 Loss: 0.293485\n",
      "\tTraining batch 29 Loss: 0.379050\n",
      "\tTraining batch 30 Loss: 0.439972\n",
      "\tTraining batch 31 Loss: 0.244262\n",
      "\tTraining batch 32 Loss: 0.321579\n",
      "\tTraining batch 33 Loss: 0.152450\n",
      "\tTraining batch 34 Loss: 0.421697\n",
      "\tTraining batch 35 Loss: 0.778419\n",
      "\tTraining batch 36 Loss: 0.489939\n",
      "\tTraining batch 37 Loss: 0.193860\n",
      "\tTraining batch 38 Loss: 0.284249\n",
      "\tTraining batch 39 Loss: 0.638980\n",
      "\tTraining batch 40 Loss: 0.525138\n",
      "\tTraining batch 41 Loss: 0.248268\n",
      "\tTraining batch 42 Loss: 0.293596\n",
      "\tTraining batch 43 Loss: 0.684812\n",
      "\tTraining batch 44 Loss: 0.342445\n",
      "\tTraining batch 45 Loss: 0.220535\n",
      "\tTraining batch 46 Loss: 0.231436\n",
      "\tTraining batch 47 Loss: 0.365520\n",
      "\tTraining batch 48 Loss: 0.602659\n",
      "\tTraining batch 49 Loss: 0.342083\n",
      "\tTraining batch 50 Loss: 0.495128\n",
      "\tTraining batch 51 Loss: 0.342664\n",
      "\tTraining batch 52 Loss: 0.410845\n",
      "\tTraining batch 53 Loss: 0.467143\n",
      "\tTraining batch 54 Loss: 0.365569\n",
      "\tTraining batch 55 Loss: 0.243300\n",
      "\tTraining batch 56 Loss: 0.400322\n",
      "\tTraining batch 57 Loss: 0.455550\n",
      "\tTraining batch 58 Loss: 0.445865\n",
      "\tTraining batch 59 Loss: 0.256203\n",
      "\tTraining batch 60 Loss: 0.298350\n",
      "\tTraining batch 61 Loss: 0.518539\n",
      "\tTraining batch 62 Loss: 0.265632\n",
      "\tTraining batch 63 Loss: 0.350035\n",
      "\tTraining batch 64 Loss: 0.406200\n",
      "\tTraining batch 65 Loss: 0.513453\n",
      "\tTraining batch 66 Loss: 0.516649\n",
      "\tTraining batch 67 Loss: 0.558547\n",
      "\tTraining batch 68 Loss: 0.320819\n",
      "\tTraining batch 69 Loss: 0.258681\n",
      "\tTraining batch 70 Loss: 0.214505\n",
      "\tTraining batch 71 Loss: 0.547356\n",
      "\tTraining batch 72 Loss: 0.442274\n",
      "\tTraining batch 73 Loss: 0.363262\n",
      "\tTraining batch 74 Loss: 0.171698\n",
      "\tTraining batch 75 Loss: 0.383403\n",
      "\tTraining batch 76 Loss: 0.310461\n",
      "\tTraining batch 77 Loss: 0.538410\n",
      "\tTraining batch 78 Loss: 0.247402\n",
      "\tTraining batch 79 Loss: 0.483270\n",
      "\tTraining batch 80 Loss: 0.247297\n",
      "\tTraining batch 81 Loss: 0.286378\n",
      "\tTraining batch 82 Loss: 0.316116\n",
      "\tTraining batch 83 Loss: 0.997674\n",
      "\tTraining batch 84 Loss: 0.318167\n",
      "\tTraining batch 85 Loss: 0.273951\n",
      "\tTraining batch 86 Loss: 0.333284\n",
      "\tTraining batch 87 Loss: 0.209285\n",
      "\tTraining batch 88 Loss: 0.465725\n",
      "\tTraining batch 89 Loss: 0.515278\n",
      "\tTraining batch 90 Loss: 0.304021\n",
      "\tTraining batch 91 Loss: 0.460162\n",
      "\tTraining batch 92 Loss: 0.306269\n",
      "\tTraining batch 93 Loss: 0.449311\n",
      "\tTraining batch 94 Loss: 0.163970\n",
      "\tTraining batch 95 Loss: 0.337011\n",
      "\tTraining batch 96 Loss: 0.423145\n",
      "\tTraining batch 97 Loss: 0.659947\n",
      "Training set: Average loss: 0.388493\n",
      "Validation set: Average loss: 0.372963, Accuracy: 1769/2070 (85%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.244279\n",
      "\tTraining batch 2 Loss: 0.411284\n",
      "\tTraining batch 3 Loss: 0.474939\n",
      "\tTraining batch 4 Loss: 0.348496\n",
      "\tTraining batch 5 Loss: 0.532128\n",
      "\tTraining batch 6 Loss: 0.453460\n",
      "\tTraining batch 7 Loss: 0.629469\n",
      "\tTraining batch 8 Loss: 0.555713\n",
      "\tTraining batch 9 Loss: 0.359358\n",
      "\tTraining batch 10 Loss: 0.676355\n",
      "\tTraining batch 11 Loss: 0.314468\n",
      "\tTraining batch 12 Loss: 0.344375\n",
      "\tTraining batch 13 Loss: 0.427409\n",
      "\tTraining batch 14 Loss: 0.382906\n",
      "\tTraining batch 15 Loss: 0.494109\n",
      "\tTraining batch 16 Loss: 0.385982\n",
      "\tTraining batch 17 Loss: 0.308290\n",
      "\tTraining batch 18 Loss: 0.319100\n",
      "\tTraining batch 19 Loss: 0.471874\n",
      "\tTraining batch 20 Loss: 0.802565\n",
      "\tTraining batch 21 Loss: 0.697445\n",
      "\tTraining batch 22 Loss: 0.621642\n",
      "\tTraining batch 23 Loss: 0.352003\n",
      "\tTraining batch 24 Loss: 0.354484\n",
      "\tTraining batch 25 Loss: 0.189508\n",
      "\tTraining batch 26 Loss: 0.262525\n",
      "\tTraining batch 27 Loss: 0.334276\n",
      "\tTraining batch 28 Loss: 0.412738\n",
      "\tTraining batch 29 Loss: 0.195712\n",
      "\tTraining batch 30 Loss: 0.306908\n",
      "\tTraining batch 31 Loss: 0.194111\n",
      "\tTraining batch 32 Loss: 0.270400\n",
      "\tTraining batch 33 Loss: 0.288469\n",
      "\tTraining batch 34 Loss: 0.550680\n",
      "\tTraining batch 35 Loss: 0.421662\n",
      "\tTraining batch 36 Loss: 0.523515\n",
      "\tTraining batch 37 Loss: 0.389037\n",
      "\tTraining batch 38 Loss: 0.304418\n",
      "\tTraining batch 39 Loss: 0.466040\n",
      "\tTraining batch 40 Loss: 0.512245\n",
      "\tTraining batch 41 Loss: 0.290872\n",
      "\tTraining batch 42 Loss: 0.182824\n",
      "\tTraining batch 43 Loss: 0.290322\n",
      "\tTraining batch 44 Loss: 0.376541\n",
      "\tTraining batch 45 Loss: 0.196181\n",
      "\tTraining batch 46 Loss: 0.268440\n",
      "\tTraining batch 47 Loss: 0.301888\n",
      "\tTraining batch 48 Loss: 0.355057\n",
      "\tTraining batch 49 Loss: 0.222837\n",
      "\tTraining batch 50 Loss: 0.704565\n",
      "\tTraining batch 51 Loss: 0.255277\n",
      "\tTraining batch 52 Loss: 0.431674\n",
      "\tTraining batch 53 Loss: 0.791306\n",
      "\tTraining batch 54 Loss: 0.388372\n",
      "\tTraining batch 55 Loss: 0.279960\n",
      "\tTraining batch 56 Loss: 0.592544\n",
      "\tTraining batch 57 Loss: 0.433942\n",
      "\tTraining batch 58 Loss: 0.446580\n",
      "\tTraining batch 59 Loss: 0.319494\n",
      "\tTraining batch 60 Loss: 0.193921\n",
      "\tTraining batch 61 Loss: 0.466116\n",
      "\tTraining batch 62 Loss: 0.432244\n",
      "\tTraining batch 63 Loss: 0.288704\n",
      "\tTraining batch 64 Loss: 0.397799\n",
      "\tTraining batch 65 Loss: 0.288707\n",
      "\tTraining batch 66 Loss: 0.209947\n",
      "\tTraining batch 67 Loss: 0.399416\n",
      "\tTraining batch 68 Loss: 0.323536\n",
      "\tTraining batch 69 Loss: 0.238925\n",
      "\tTraining batch 70 Loss: 0.233938\n",
      "\tTraining batch 71 Loss: 0.416650\n",
      "\tTraining batch 72 Loss: 0.586590\n",
      "\tTraining batch 73 Loss: 0.557723\n",
      "\tTraining batch 74 Loss: 0.219999\n",
      "\tTraining batch 75 Loss: 0.293024\n",
      "\tTraining batch 76 Loss: 0.416821\n",
      "\tTraining batch 77 Loss: 0.339723\n",
      "\tTraining batch 78 Loss: 0.235622\n",
      "\tTraining batch 79 Loss: 0.372116\n",
      "\tTraining batch 80 Loss: 0.373829\n",
      "\tTraining batch 81 Loss: 0.304510\n",
      "\tTraining batch 82 Loss: 0.341951\n",
      "\tTraining batch 83 Loss: 0.424552\n",
      "\tTraining batch 84 Loss: 0.308601\n",
      "\tTraining batch 85 Loss: 0.479358\n",
      "\tTraining batch 86 Loss: 0.443444\n",
      "\tTraining batch 87 Loss: 0.161672\n",
      "\tTraining batch 88 Loss: 0.358224\n",
      "\tTraining batch 89 Loss: 0.344152\n",
      "\tTraining batch 90 Loss: 0.215271\n",
      "\tTraining batch 91 Loss: 0.298905\n",
      "\tTraining batch 92 Loss: 0.248899\n",
      "\tTraining batch 93 Loss: 0.686693\n",
      "\tTraining batch 94 Loss: 0.311918\n",
      "\tTraining batch 95 Loss: 0.315654\n",
      "\tTraining batch 96 Loss: 0.283182\n",
      "\tTraining batch 97 Loss: 0.288810\n",
      "Training set: Average loss: 0.379528\n",
      "Validation set: Average loss: 0.322976, Accuracy: 1808/2070 (87%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.366350\n",
      "\tTraining batch 2 Loss: 0.479037\n",
      "\tTraining batch 3 Loss: 0.426794\n",
      "\tTraining batch 4 Loss: 0.338788\n",
      "\tTraining batch 5 Loss: 0.424426\n",
      "\tTraining batch 6 Loss: 0.591338\n",
      "\tTraining batch 7 Loss: 0.532639\n",
      "\tTraining batch 8 Loss: 0.460129\n",
      "\tTraining batch 9 Loss: 0.388114\n",
      "\tTraining batch 10 Loss: 0.423751\n",
      "\tTraining batch 11 Loss: 0.276048\n",
      "\tTraining batch 12 Loss: 0.382290\n",
      "\tTraining batch 13 Loss: 0.300998\n",
      "\tTraining batch 14 Loss: 0.284223\n",
      "\tTraining batch 15 Loss: 0.487328\n",
      "\tTraining batch 16 Loss: 0.138437\n",
      "\tTraining batch 17 Loss: 0.230719\n",
      "\tTraining batch 18 Loss: 0.306041\n",
      "\tTraining batch 19 Loss: 0.119991\n",
      "\tTraining batch 20 Loss: 0.801619\n",
      "\tTraining batch 21 Loss: 0.656905\n",
      "\tTraining batch 22 Loss: 0.323798\n",
      "\tTraining batch 23 Loss: 0.308831\n",
      "\tTraining batch 24 Loss: 0.411205\n",
      "\tTraining batch 25 Loss: 0.347338\n",
      "\tTraining batch 26 Loss: 0.310388\n",
      "\tTraining batch 27 Loss: 0.363695\n",
      "\tTraining batch 28 Loss: 0.277612\n",
      "\tTraining batch 29 Loss: 0.138703\n",
      "\tTraining batch 30 Loss: 0.297326\n",
      "\tTraining batch 31 Loss: 0.086049\n",
      "\tTraining batch 32 Loss: 0.609612\n",
      "\tTraining batch 33 Loss: 0.200382\n",
      "\tTraining batch 34 Loss: 0.489054\n",
      "\tTraining batch 35 Loss: 0.583767\n",
      "\tTraining batch 36 Loss: 0.593558\n",
      "\tTraining batch 37 Loss: 0.142777\n",
      "\tTraining batch 38 Loss: 0.309475\n",
      "\tTraining batch 39 Loss: 0.584462\n",
      "\tTraining batch 40 Loss: 0.439640\n",
      "\tTraining batch 41 Loss: 0.549593\n",
      "\tTraining batch 42 Loss: 0.228761\n",
      "\tTraining batch 43 Loss: 0.499569\n",
      "\tTraining batch 44 Loss: 0.551139\n",
      "\tTraining batch 45 Loss: 0.438321\n",
      "\tTraining batch 46 Loss: 0.375304\n",
      "\tTraining batch 47 Loss: 0.469887\n",
      "\tTraining batch 48 Loss: 0.317255\n",
      "\tTraining batch 49 Loss: 0.174518\n",
      "\tTraining batch 50 Loss: 0.316042\n",
      "\tTraining batch 51 Loss: 0.327436\n",
      "\tTraining batch 52 Loss: 0.426222\n",
      "\tTraining batch 53 Loss: 0.408835\n",
      "\tTraining batch 54 Loss: 0.450441\n",
      "\tTraining batch 55 Loss: 0.230023\n",
      "\tTraining batch 56 Loss: 0.293068\n",
      "\tTraining batch 57 Loss: 0.324776\n",
      "\tTraining batch 58 Loss: 0.342734\n",
      "\tTraining batch 59 Loss: 0.389863\n",
      "\tTraining batch 60 Loss: 0.128036\n",
      "\tTraining batch 61 Loss: 0.526252\n",
      "\tTraining batch 62 Loss: 0.292113\n",
      "\tTraining batch 63 Loss: 0.448348\n",
      "\tTraining batch 64 Loss: 0.317587\n",
      "\tTraining batch 65 Loss: 0.391468\n",
      "\tTraining batch 66 Loss: 0.398961\n",
      "\tTraining batch 67 Loss: 0.263825\n",
      "\tTraining batch 68 Loss: 0.580359\n",
      "\tTraining batch 69 Loss: 0.293371\n",
      "\tTraining batch 70 Loss: 0.363753\n",
      "\tTraining batch 71 Loss: 0.475899\n",
      "\tTraining batch 72 Loss: 0.327971\n",
      "\tTraining batch 73 Loss: 0.281733\n",
      "\tTraining batch 74 Loss: 0.181307\n",
      "\tTraining batch 75 Loss: 0.492627\n",
      "\tTraining batch 76 Loss: 0.211935\n",
      "\tTraining batch 77 Loss: 0.300924\n",
      "\tTraining batch 78 Loss: 0.343849\n",
      "\tTraining batch 79 Loss: 0.511081\n",
      "\tTraining batch 80 Loss: 0.155392\n",
      "\tTraining batch 81 Loss: 0.247341\n",
      "\tTraining batch 82 Loss: 0.176595\n",
      "\tTraining batch 83 Loss: 0.783162\n",
      "\tTraining batch 84 Loss: 0.365676\n",
      "\tTraining batch 85 Loss: 0.278784\n",
      "\tTraining batch 86 Loss: 0.372050\n",
      "\tTraining batch 87 Loss: 0.223570\n",
      "\tTraining batch 88 Loss: 0.286666\n",
      "\tTraining batch 89 Loss: 0.283513\n",
      "\tTraining batch 90 Loss: 0.349429\n",
      "\tTraining batch 91 Loss: 0.304948\n",
      "\tTraining batch 92 Loss: 0.333848\n",
      "\tTraining batch 93 Loss: 0.532034\n",
      "\tTraining batch 94 Loss: 0.209852\n",
      "\tTraining batch 95 Loss: 0.263980\n",
      "\tTraining batch 96 Loss: 0.339719\n",
      "\tTraining batch 97 Loss: 0.373313\n",
      "Training set: Average loss: 0.364523\n",
      "Validation set: Average loss: 0.361113, Accuracy: 1761/2070 (85%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.220803\n",
      "\tTraining batch 2 Loss: 0.477233\n",
      "\tTraining batch 3 Loss: 0.311413\n",
      "\tTraining batch 4 Loss: 0.300561\n",
      "\tTraining batch 5 Loss: 0.430485\n",
      "\tTraining batch 6 Loss: 0.344449\n",
      "\tTraining batch 7 Loss: 0.659822\n",
      "\tTraining batch 8 Loss: 0.519992\n",
      "\tTraining batch 9 Loss: 0.503095\n",
      "\tTraining batch 10 Loss: 0.363750\n",
      "\tTraining batch 11 Loss: 0.232156\n",
      "\tTraining batch 12 Loss: 0.553281\n",
      "\tTraining batch 13 Loss: 0.225427\n",
      "\tTraining batch 14 Loss: 0.315008\n",
      "\tTraining batch 15 Loss: 0.439913\n",
      "\tTraining batch 16 Loss: 0.324528\n",
      "\tTraining batch 17 Loss: 0.300487\n",
      "\tTraining batch 18 Loss: 0.310955\n",
      "\tTraining batch 19 Loss: 0.191985\n",
      "\tTraining batch 20 Loss: 0.445387\n",
      "\tTraining batch 21 Loss: 0.536483\n",
      "\tTraining batch 22 Loss: 0.352338\n",
      "\tTraining batch 23 Loss: 0.425213\n",
      "\tTraining batch 24 Loss: 0.413260\n",
      "\tTraining batch 25 Loss: 0.471907\n",
      "\tTraining batch 26 Loss: 0.306643\n",
      "\tTraining batch 27 Loss: 0.286042\n",
      "\tTraining batch 28 Loss: 0.378200\n",
      "\tTraining batch 29 Loss: 0.255516\n",
      "\tTraining batch 30 Loss: 0.265130\n",
      "\tTraining batch 31 Loss: 0.202205\n",
      "\tTraining batch 32 Loss: 0.354193\n",
      "\tTraining batch 33 Loss: 0.285392\n",
      "\tTraining batch 34 Loss: 0.325415\n",
      "\tTraining batch 35 Loss: 0.460373\n",
      "\tTraining batch 36 Loss: 0.348909\n",
      "\tTraining batch 37 Loss: 0.218427\n",
      "\tTraining batch 38 Loss: 0.511944\n",
      "\tTraining batch 39 Loss: 0.266334\n",
      "\tTraining batch 40 Loss: 0.438145\n",
      "\tTraining batch 41 Loss: 0.591584\n",
      "\tTraining batch 42 Loss: 0.197931\n",
      "\tTraining batch 43 Loss: 0.390732\n",
      "\tTraining batch 44 Loss: 0.442942\n",
      "\tTraining batch 45 Loss: 0.403888\n",
      "\tTraining batch 46 Loss: 0.385418\n",
      "\tTraining batch 47 Loss: 0.282973\n",
      "\tTraining batch 48 Loss: 0.314308\n",
      "\tTraining batch 49 Loss: 0.315137\n",
      "\tTraining batch 50 Loss: 0.377720\n",
      "\tTraining batch 51 Loss: 0.295648\n",
      "\tTraining batch 52 Loss: 0.461947\n",
      "\tTraining batch 53 Loss: 0.493177\n",
      "\tTraining batch 54 Loss: 0.269950\n",
      "\tTraining batch 55 Loss: 0.218526\n",
      "\tTraining batch 56 Loss: 0.307494\n",
      "\tTraining batch 57 Loss: 0.520538\n",
      "\tTraining batch 58 Loss: 0.280340\n",
      "\tTraining batch 59 Loss: 0.220285\n",
      "\tTraining batch 60 Loss: 0.134850\n",
      "\tTraining batch 61 Loss: 0.507477\n",
      "\tTraining batch 62 Loss: 0.247829\n",
      "\tTraining batch 63 Loss: 0.336438\n",
      "\tTraining batch 64 Loss: 0.416931\n",
      "\tTraining batch 65 Loss: 0.427847\n",
      "\tTraining batch 66 Loss: 0.525960\n",
      "\tTraining batch 67 Loss: 0.402305\n",
      "\tTraining batch 68 Loss: 0.434035\n",
      "\tTraining batch 69 Loss: 0.268725\n",
      "\tTraining batch 70 Loss: 0.327933\n",
      "\tTraining batch 71 Loss: 0.429039\n",
      "\tTraining batch 72 Loss: 0.280596\n",
      "\tTraining batch 73 Loss: 0.243282\n",
      "\tTraining batch 74 Loss: 0.323524\n",
      "\tTraining batch 75 Loss: 0.349095\n",
      "\tTraining batch 76 Loss: 0.243074\n",
      "\tTraining batch 77 Loss: 0.281389\n",
      "\tTraining batch 78 Loss: 0.248279\n",
      "\tTraining batch 79 Loss: 0.222954\n",
      "\tTraining batch 80 Loss: 0.205398\n",
      "\tTraining batch 81 Loss: 0.493531\n",
      "\tTraining batch 82 Loss: 0.186350\n",
      "\tTraining batch 83 Loss: 0.841920\n",
      "\tTraining batch 84 Loss: 0.267409\n",
      "\tTraining batch 85 Loss: 0.293389\n",
      "\tTraining batch 86 Loss: 0.271385\n",
      "\tTraining batch 87 Loss: 0.121241\n",
      "\tTraining batch 88 Loss: 0.288187\n",
      "\tTraining batch 89 Loss: 0.232884\n",
      "\tTraining batch 90 Loss: 0.399041\n",
      "\tTraining batch 91 Loss: 0.337699\n",
      "\tTraining batch 92 Loss: 0.399526\n",
      "\tTraining batch 93 Loss: 0.358096\n",
      "\tTraining batch 94 Loss: 0.205383\n",
      "\tTraining batch 95 Loss: 0.504127\n",
      "\tTraining batch 96 Loss: 0.219913\n",
      "\tTraining batch 97 Loss: 0.346052\n",
      "Training set: Average loss: 0.350169\n",
      "Validation set: Average loss: 0.343731, Accuracy: 1808/2070 (87%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.209883\n",
      "\tTraining batch 2 Loss: 0.276846\n",
      "\tTraining batch 3 Loss: 0.533821\n",
      "\tTraining batch 4 Loss: 0.337846\n",
      "\tTraining batch 5 Loss: 0.293441\n",
      "\tTraining batch 6 Loss: 0.325367\n",
      "\tTraining batch 7 Loss: 0.618391\n",
      "\tTraining batch 8 Loss: 0.356771\n",
      "\tTraining batch 9 Loss: 0.391375\n",
      "\tTraining batch 10 Loss: 0.479676\n",
      "\tTraining batch 11 Loss: 0.271055\n",
      "\tTraining batch 12 Loss: 0.326859\n",
      "\tTraining batch 13 Loss: 0.376835\n",
      "\tTraining batch 14 Loss: 0.194762\n",
      "\tTraining batch 15 Loss: 0.347651\n",
      "\tTraining batch 16 Loss: 0.199998\n",
      "\tTraining batch 17 Loss: 0.275510\n",
      "\tTraining batch 18 Loss: 0.309902\n",
      "\tTraining batch 19 Loss: 0.249992\n",
      "\tTraining batch 20 Loss: 0.377491\n",
      "\tTraining batch 21 Loss: 0.582862\n",
      "\tTraining batch 22 Loss: 0.432559\n",
      "\tTraining batch 23 Loss: 0.444411\n",
      "\tTraining batch 24 Loss: 0.462822\n",
      "\tTraining batch 25 Loss: 0.348625\n",
      "\tTraining batch 26 Loss: 0.327902\n",
      "\tTraining batch 27 Loss: 0.340296\n",
      "\tTraining batch 28 Loss: 0.287648\n",
      "\tTraining batch 29 Loss: 0.183160\n",
      "\tTraining batch 30 Loss: 0.407221\n",
      "\tTraining batch 31 Loss: 0.186506\n",
      "\tTraining batch 32 Loss: 0.286265\n",
      "\tTraining batch 33 Loss: 0.373940\n",
      "\tTraining batch 34 Loss: 0.383107\n",
      "\tTraining batch 35 Loss: 0.816134\n",
      "\tTraining batch 36 Loss: 0.362332\n",
      "\tTraining batch 37 Loss: 0.118966\n",
      "\tTraining batch 38 Loss: 0.358647\n",
      "\tTraining batch 39 Loss: 0.412404\n",
      "\tTraining batch 40 Loss: 0.385484\n",
      "\tTraining batch 41 Loss: 0.659201\n",
      "\tTraining batch 42 Loss: 0.407477\n",
      "\tTraining batch 43 Loss: 0.632557\n",
      "\tTraining batch 44 Loss: 0.414866\n",
      "\tTraining batch 45 Loss: 0.359107\n",
      "\tTraining batch 46 Loss: 0.376020\n",
      "\tTraining batch 47 Loss: 0.397153\n",
      "\tTraining batch 48 Loss: 0.760782\n",
      "\tTraining batch 49 Loss: 0.186580\n",
      "\tTraining batch 50 Loss: 0.362865\n",
      "\tTraining batch 51 Loss: 0.466385\n",
      "\tTraining batch 52 Loss: 0.419261\n",
      "\tTraining batch 53 Loss: 0.699896\n",
      "\tTraining batch 54 Loss: 0.513100\n",
      "\tTraining batch 55 Loss: 0.310452\n",
      "\tTraining batch 56 Loss: 0.282909\n",
      "\tTraining batch 57 Loss: 0.434314\n",
      "\tTraining batch 58 Loss: 0.434545\n",
      "\tTraining batch 59 Loss: 0.241124\n",
      "\tTraining batch 60 Loss: 0.151072\n",
      "\tTraining batch 61 Loss: 0.623078\n",
      "\tTraining batch 62 Loss: 0.339618\n",
      "\tTraining batch 63 Loss: 0.295862\n",
      "\tTraining batch 64 Loss: 0.380237\n",
      "\tTraining batch 65 Loss: 0.380387\n",
      "\tTraining batch 66 Loss: 0.471543\n",
      "\tTraining batch 67 Loss: 0.312667\n",
      "\tTraining batch 68 Loss: 0.441402\n",
      "\tTraining batch 69 Loss: 0.141712\n",
      "\tTraining batch 70 Loss: 0.235844\n",
      "\tTraining batch 71 Loss: 0.348620\n",
      "\tTraining batch 72 Loss: 0.271063\n",
      "\tTraining batch 73 Loss: 0.278181\n",
      "\tTraining batch 74 Loss: 0.255703\n",
      "\tTraining batch 75 Loss: 0.246994\n",
      "\tTraining batch 76 Loss: 0.351132\n",
      "\tTraining batch 77 Loss: 0.407582\n",
      "\tTraining batch 78 Loss: 0.268621\n",
      "\tTraining batch 79 Loss: 0.338017\n",
      "\tTraining batch 80 Loss: 0.257367\n",
      "\tTraining batch 81 Loss: 0.290235\n",
      "\tTraining batch 82 Loss: 0.241926\n",
      "\tTraining batch 83 Loss: 0.737905\n",
      "\tTraining batch 84 Loss: 0.371290\n",
      "\tTraining batch 85 Loss: 0.247158\n",
      "\tTraining batch 86 Loss: 0.472996\n",
      "\tTraining batch 87 Loss: 0.133153\n",
      "\tTraining batch 88 Loss: 0.306526\n",
      "\tTraining batch 89 Loss: 0.529192\n",
      "\tTraining batch 90 Loss: 0.461814\n",
      "\tTraining batch 91 Loss: 0.433874\n",
      "\tTraining batch 92 Loss: 0.553114\n",
      "\tTraining batch 93 Loss: 0.240793\n",
      "\tTraining batch 94 Loss: 0.255182\n",
      "\tTraining batch 95 Loss: 0.238554\n",
      "\tTraining batch 96 Loss: 0.582628\n",
      "\tTraining batch 97 Loss: 0.244817\n",
      "Training set: Average loss: 0.368569\n",
      "Validation set: Average loss: 0.314424, Accuracy: 1806/2070 (87%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.198543\n",
      "\tTraining batch 2 Loss: 0.342773\n",
      "\tTraining batch 3 Loss: 0.279027\n",
      "\tTraining batch 4 Loss: 0.251794\n",
      "\tTraining batch 5 Loss: 0.408356\n",
      "\tTraining batch 6 Loss: 0.282659\n",
      "\tTraining batch 7 Loss: 0.522010\n",
      "\tTraining batch 8 Loss: 0.294511\n",
      "\tTraining batch 9 Loss: 0.214349\n",
      "\tTraining batch 10 Loss: 0.525822\n",
      "\tTraining batch 11 Loss: 0.272254\n",
      "\tTraining batch 12 Loss: 0.408731\n",
      "\tTraining batch 13 Loss: 0.394853\n",
      "\tTraining batch 14 Loss: 0.422344\n",
      "\tTraining batch 15 Loss: 0.331913\n",
      "\tTraining batch 16 Loss: 0.286899\n",
      "\tTraining batch 17 Loss: 0.237632\n",
      "\tTraining batch 18 Loss: 0.209974\n",
      "\tTraining batch 19 Loss: 0.254775\n",
      "\tTraining batch 20 Loss: 0.503900\n",
      "\tTraining batch 21 Loss: 0.545219\n",
      "\tTraining batch 22 Loss: 0.319927\n",
      "\tTraining batch 23 Loss: 0.435398\n",
      "\tTraining batch 24 Loss: 0.360692\n",
      "\tTraining batch 25 Loss: 0.199957\n",
      "\tTraining batch 26 Loss: 0.237136\n",
      "\tTraining batch 27 Loss: 0.477466\n",
      "\tTraining batch 28 Loss: 0.287060\n",
      "\tTraining batch 29 Loss: 0.294048\n",
      "\tTraining batch 30 Loss: 0.352303\n",
      "\tTraining batch 31 Loss: 0.233777\n",
      "\tTraining batch 32 Loss: 0.358778\n",
      "\tTraining batch 33 Loss: 0.471434\n",
      "\tTraining batch 34 Loss: 0.534990\n",
      "\tTraining batch 35 Loss: 0.475706\n",
      "\tTraining batch 36 Loss: 0.530242\n",
      "\tTraining batch 37 Loss: 0.100592\n",
      "\tTraining batch 38 Loss: 0.292958\n",
      "\tTraining batch 39 Loss: 0.459630\n",
      "\tTraining batch 40 Loss: 0.305594\n",
      "\tTraining batch 41 Loss: 0.316683\n",
      "\tTraining batch 42 Loss: 0.287807\n",
      "\tTraining batch 43 Loss: 0.370448\n",
      "\tTraining batch 44 Loss: 0.366618\n",
      "\tTraining batch 45 Loss: 0.151941\n",
      "\tTraining batch 46 Loss: 0.248288\n",
      "\tTraining batch 47 Loss: 0.341411\n",
      "\tTraining batch 48 Loss: 0.702981\n",
      "\tTraining batch 49 Loss: 0.315285\n",
      "\tTraining batch 50 Loss: 0.514550\n",
      "\tTraining batch 51 Loss: 0.167353\n",
      "\tTraining batch 52 Loss: 0.413953\n",
      "\tTraining batch 53 Loss: 0.793290\n",
      "\tTraining batch 54 Loss: 0.313036\n",
      "\tTraining batch 55 Loss: 0.302047\n",
      "\tTraining batch 56 Loss: 0.255233\n",
      "\tTraining batch 57 Loss: 0.519059\n",
      "\tTraining batch 58 Loss: 0.214671\n",
      "\tTraining batch 59 Loss: 0.188025\n",
      "\tTraining batch 60 Loss: 0.323939\n",
      "\tTraining batch 61 Loss: 0.422995\n",
      "\tTraining batch 62 Loss: 0.249063\n",
      "\tTraining batch 63 Loss: 0.294737\n",
      "\tTraining batch 64 Loss: 0.297142\n",
      "\tTraining batch 65 Loss: 0.389881\n",
      "\tTraining batch 66 Loss: 0.430375\n",
      "\tTraining batch 67 Loss: 0.359649\n",
      "\tTraining batch 68 Loss: 0.426520\n",
      "\tTraining batch 69 Loss: 0.333606\n",
      "\tTraining batch 70 Loss: 0.150488\n",
      "\tTraining batch 71 Loss: 0.414554\n",
      "\tTraining batch 72 Loss: 0.214707\n",
      "\tTraining batch 73 Loss: 0.348566\n",
      "\tTraining batch 74 Loss: 0.168107\n",
      "\tTraining batch 75 Loss: 0.542982\n",
      "\tTraining batch 76 Loss: 0.244433\n",
      "\tTraining batch 77 Loss: 0.343477\n",
      "\tTraining batch 78 Loss: 0.223168\n",
      "\tTraining batch 79 Loss: 0.470344\n",
      "\tTraining batch 80 Loss: 0.254645\n",
      "\tTraining batch 81 Loss: 0.345867\n",
      "\tTraining batch 82 Loss: 0.274451\n",
      "\tTraining batch 83 Loss: 0.314708\n",
      "\tTraining batch 84 Loss: 0.183986\n",
      "\tTraining batch 85 Loss: 0.276444\n",
      "\tTraining batch 86 Loss: 0.290203\n",
      "\tTraining batch 87 Loss: 0.137299\n",
      "\tTraining batch 88 Loss: 0.338196\n",
      "\tTraining batch 89 Loss: 0.182849\n",
      "\tTraining batch 90 Loss: 0.282311\n",
      "\tTraining batch 91 Loss: 0.368432\n",
      "\tTraining batch 92 Loss: 0.218186\n",
      "\tTraining batch 93 Loss: 0.293349\n",
      "\tTraining batch 94 Loss: 0.182244\n",
      "\tTraining batch 95 Loss: 0.158486\n",
      "\tTraining batch 96 Loss: 0.206721\n",
      "\tTraining batch 97 Loss: 0.345794\n",
      "Training set: Average loss: 0.329913\n",
      "Validation set: Average loss: 0.360694, Accuracy: 1807/2070 (87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Specify the loss criteria\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Train over 10 epochs (We restrict to 10 for time issues)\n",
    "epochs = 10\n",
    "print('Training on', device)\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Loss History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(epoch_nums, training_loss)\n",
    "plt.plot(epoch_nums, validation_loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions from test set...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining Labels and Predictions\n",
    "truelabels = []\n",
    "predictions = []\n",
    "model.eval()\n",
    "print(\"Getting predictions from test set...\")\n",
    "for data, target in test_loader:\n",
    "    for label in target.data.numpy():\n",
    "        truelabels.append(label)\n",
    "    for prediction in model(data).data.numpy().argmax(1):\n",
    "        predictions.append(prediction) \n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(truelabels, predictions)\n",
    "tick_marks = np.arange(len(classes))\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = classes, columns = classes)\n",
    "plt.figure(figsize = (7,7))\n",
    "sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n",
    "plt.xlabel(\"Predicted Shape\", fontsize = 20)\n",
    "plt.ylabel(\"True Shape\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a huge mislassification among dog and cats. It is somewhat expected, since dogs and cat looks alike, and we need a well deeper CNN to properly classify them. \n",
    "\n",
    "\n",
    "A somewhat unexpected classification error is the superposition of Flowers upon Dog/Cats, instead of fruits that are well classified!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we insert a code to cancel the images we have resized and saved into the '../working' folder due to the fact that Kaggle limits to 500 the number of output files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"../working/data/natural_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you enjoyed the reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
